{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "aADyAgXWU2jA"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creation of a corpus (kind of paragraph for)\n",
        "corpus = \"Hello to all,this is the beginning class for nltk. Going to explain the basic of NLP.\""
      ],
      "metadata": {
        "id": "z5b-FMJIoQUL"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenization : convert a sentence into paragraph\n",
        "#nltk.download('punkt_tab')\n",
        "from nltk.tokenize import sent_tokenize\n",
        "documents=sent_tokenize(corpus)\n",
        "print(documents)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvUhNyufv-KH",
        "outputId": "22e47257-753a-423c-c011-28358eac688d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Hello to all,this is the beginning class for nltk.', 'Going to explain the basic of NLP.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in documents:\n",
        "    print(sentence)\n",
        "\n",
        "#Tokenization\n",
        "# paragraph --> words\n",
        "# sentence --> words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KpM-0DwwgF3",
        "outputId": "70471214-a9a3-419f-fc64-653d8cc4ff70"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello to all,this is the beginning class for nltk.\n",
            "Going to explain the basic of NLP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "for word in documents:\n",
        "    print(word_tokenize(sentence))\n",
        "print(word)"
      ],
      "metadata": {
        "id": "jUEe5_KFwqH5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f589f0b5-d430-43ec-fadc-afa8212ca40d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Going', 'to', 'explain', 'the', 'basic', 'of', 'NLP', '.']\n",
            "['Going', 'to', 'explain', 'the', 'basic', 'of', 'NLP', '.']\n",
            "Going to explain the basic of NLP.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# another to display everything\n",
        "word_tokenize(corpus) # displaying all words in token form"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCT3ewdAAqtV",
        "outputId": "d4be5f75-bf38-479e-81f1-f8e51ff8c62c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'to',\n",
              " 'all',\n",
              " ',',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'beginning',\n",
              " 'class',\n",
              " 'for',\n",
              " 'nltk',\n",
              " '.',\n",
              " 'Going',\n",
              " 'to',\n",
              " 'explain',\n",
              " 'the',\n",
              " 'basic',\n",
              " 'of',\n",
              " 'NLP',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import wordpunct_tokenize\n",
        "# Why using word_punct ?\n",
        "#In case we want to make sure punctuation is created as a single word\n",
        "wordpunct_tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wenf_9jVA5cT",
        "outputId": "caa72939-0ef7-4d5a-d8fb-ccf785d77b8e"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'to',\n",
              " 'all',\n",
              " ',',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'beginning',\n",
              " 'class',\n",
              " 'for',\n",
              " 'nltk',\n",
              " '.',\n",
              " 'Going',\n",
              " 'to',\n",
              " 'explain',\n",
              " 'the',\n",
              " 'basic',\n",
              " 'of',\n",
              " 'NLP',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import TreebankWordTokenizer\n",
        "tokenizer = TreebankWordTokenizer()\n",
        "#what happen with Treebank token-  ?\n",
        "# full stop is not considered as a separate word\n",
        "# but the last full stop is considered as one\n",
        "tokenizer.tokenize(corpus)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u3fY0Q7lBwsh",
        "outputId": "496dea92-11d7-4c98-fb71-a8b772af5427"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hello',\n",
              " 'to',\n",
              " 'all',\n",
              " ',',\n",
              " 'this',\n",
              " 'is',\n",
              " 'the',\n",
              " 'beginning',\n",
              " 'class',\n",
              " 'for',\n",
              " 'nltk.',\n",
              " 'Going',\n",
              " 'to',\n",
              " 'explain',\n",
              " 'the',\n",
              " 'basic',\n",
              " 'of',\n",
              " 'NLP',\n",
              " '.']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "vldTKdaMF6Ho"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "STEMMING<br>\n",
        "reducing a word to its word stem (root)"
      ],
      "metadata": {
        "id": "btKBfsHiGfmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#let's consider bunch of word\n",
        "words = [\"eating\",\"eats\",\"eaten\",\"ate\"] # about eating\n",
        "\n",
        "\n",
        "#Use porterStemmer\n",
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()\n",
        "for word in words:\n",
        "    print(word+\"----->\"+ps.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y_ZiBrNFGhMr",
        "outputId": "843003cf-5ecc-4c42-9c55-fde05fe2d9a7"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating----->eat\n",
            "eats----->eat\n",
            "eaten----->eaten\n",
            "ate----->ate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "r7mXaA2YHlm8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "stemmer have some majors drawbacks in reducing the word to its stem\n",
        "<br> example of \"eaten\" or even \"congratulations\"\n",
        "<br> for that we have others stemming techniques that we can dive in\n"
      ],
      "metadata": {
        "id": "a8HgU0_wJxUW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "rs = RegexpStemmer('ing$|s$|e$|able$|en$|ion$|',min=4)\n",
        "for word in words:\n",
        "    print(word+\"---->\"+rs.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3hl64En9KHG8",
        "outputId": "b80b11fa-c0d5-46aa-a08c-0b3504f3e535"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eat\n",
            "ate---->ate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "ss = SnowballStemmer('english')\n",
        "for word in words:\n",
        "    print(word+\"---->\"+ss.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_MTB_eYCKYk0",
        "outputId": "fc109948-c2f8-44be-a103-74e27aeb44e6"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eaten\n",
            "ate---->ate\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fmnDhXbNK9la"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization"
      ],
      "metadata": {
        "id": "bdny81J1Lqh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatizer.lemmatize(\"going\",pos='v')\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "D1f-Sh5jLsI6",
        "outputId": "8902a40a-d5bb-4099-d5eb-efa8348af021"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "parameters in lemmatizer :<br>\n",
        "-a : adjective\n",
        "-v : verbs\n",
        "-n : nouns"
      ],
      "metadata": {
        "id": "TY7LcP4DrPLY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tUoscN-Aqac9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Removing ----Stopwords"
      ],
      "metadata": {
        "id": "XTk7CXvgbKxL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "para=\"\"\"\n",
        "There is something truly magical about spending time in nature. Whether it's a quiet walk through a forest, the sound of waves crashing on the shore, or the sight of a sunset painting the sky in hues of orange and pink, nature has a way of calming the mind and uplifting the spirit.\n",
        "\n",
        "Every season brings its own wonders—spring blossoms, summer warmth, autumn leaves, and winter's serene silence. Even in the busiest of lives, taking a moment to appreciate the natural world can bring peace and perspective.\n",
        "\n",
        "So next time you feel overwhelmed, step outside, take a deep breath, and let nature remind you of life's simple yet profound beauty.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "OCetz4P5bNeB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "ps = PorterStemmer()"
      ],
      "metadata": {
        "id": "ZjyMDepFbqgo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9nnK46hUbwNI",
        "outputId": "e2ac23de-5071-45e9-a4f6-79db866a1c38"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english') # take all majors languages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqJ13x01b6yi",
        "outputId": "503c6d81-2ce8-42bc-86d9-1171f4f03924"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['a',\n",
              " 'about',\n",
              " 'above',\n",
              " 'after',\n",
              " 'again',\n",
              " 'against',\n",
              " 'ain',\n",
              " 'all',\n",
              " 'am',\n",
              " 'an',\n",
              " 'and',\n",
              " 'any',\n",
              " 'are',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'as',\n",
              " 'at',\n",
              " 'be',\n",
              " 'because',\n",
              " 'been',\n",
              " 'before',\n",
              " 'being',\n",
              " 'below',\n",
              " 'between',\n",
              " 'both',\n",
              " 'but',\n",
              " 'by',\n",
              " 'can',\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'd',\n",
              " 'did',\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'do',\n",
              " 'does',\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'doing',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'down',\n",
              " 'during',\n",
              " 'each',\n",
              " 'few',\n",
              " 'for',\n",
              " 'from',\n",
              " 'further',\n",
              " 'had',\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'has',\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'have',\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'having',\n",
              " 'he',\n",
              " \"he'd\",\n",
              " \"he'll\",\n",
              " 'her',\n",
              " 'here',\n",
              " 'hers',\n",
              " 'herself',\n",
              " \"he's\",\n",
              " 'him',\n",
              " 'himself',\n",
              " 'his',\n",
              " 'how',\n",
              " 'i',\n",
              " \"i'd\",\n",
              " 'if',\n",
              " \"i'll\",\n",
              " \"i'm\",\n",
              " 'in',\n",
              " 'into',\n",
              " 'is',\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'it',\n",
              " \"it'd\",\n",
              " \"it'll\",\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " \"i've\",\n",
              " 'just',\n",
              " 'll',\n",
              " 'm',\n",
              " 'ma',\n",
              " 'me',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'more',\n",
              " 'most',\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'my',\n",
              " 'myself',\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'now',\n",
              " 'o',\n",
              " 'of',\n",
              " 'off',\n",
              " 'on',\n",
              " 'once',\n",
              " 'only',\n",
              " 'or',\n",
              " 'other',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'out',\n",
              " 'over',\n",
              " 'own',\n",
              " 're',\n",
              " 's',\n",
              " 'same',\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'she',\n",
              " \"she'd\",\n",
              " \"she'll\",\n",
              " \"she's\",\n",
              " 'should',\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " \"should've\",\n",
              " 'so',\n",
              " 'some',\n",
              " 'such',\n",
              " 't',\n",
              " 'than',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'the',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'them',\n",
              " 'themselves',\n",
              " 'then',\n",
              " 'there',\n",
              " 'these',\n",
              " 'they',\n",
              " \"they'd\",\n",
              " \"they'll\",\n",
              " \"they're\",\n",
              " \"they've\",\n",
              " 'this',\n",
              " 'those',\n",
              " 'through',\n",
              " 'to',\n",
              " 'too',\n",
              " 'under',\n",
              " 'until',\n",
              " 'up',\n",
              " 've',\n",
              " 'very',\n",
              " 'was',\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'we',\n",
              " \"we'd\",\n",
              " \"we'll\",\n",
              " \"we're\",\n",
              " 'were',\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " \"we've\",\n",
              " 'what',\n",
              " 'when',\n",
              " 'where',\n",
              " 'which',\n",
              " 'while',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'why',\n",
              " 'will',\n",
              " 'with',\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\",\n",
              " 'y',\n",
              " 'you',\n",
              " \"you'd\",\n",
              " \"you'll\",\n",
              " 'your',\n",
              " \"you're\",\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " \"you've\"]"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bTmd7WcUcB8Q",
        "outputId": "0c940a74-4ff8-41b7-c6c4-5dd8cecf3a25"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nThere is something truly magical about spending time in nature.',\n",
              " \"Whether it's a quiet walk through a forest, the sound of waves crashing on the shore, or the sight of a sunset painting the sky in hues of orange and pink, nature has a way of calming the mind and uplifting the spirit.\",\n",
              " \"Every season brings its own wonders—spring blossoms, summer warmth, autumn leaves, and winter's serene silence.\",\n",
              " 'Even in the busiest of lives, taking a moment to appreciate the natural world can bring peace and perspective.',\n",
              " \"So next time you feel overwhelmed, step outside, take a deep breath, and let nature remind you of life's simple yet profound beauty.\"]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sentence=nltk.sent_tokenize(para)\n",
        "type(sentence)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tg6gtd6kcY17",
        "outputId": "1c8ef94f-0ed6-4755-caea-a4a07d14a2ce"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        " #Apply stopwords and filter and then apply stemming\n",
        "\n",
        "for i in range(len(sentence)):\n",
        "  words = nltk.word_tokenize(sentence[i])\n",
        "  words = [ps.stem(word) for word in words if word not in set(stopwords.words('english'))]"
      ],
      "metadata": {
        "id": "bNg3acsjch6P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}