# üß† GenIA & NLP ‚Äì Bases et Projets avec LangChain

Bienvenue dans ce d√©p√¥t consacr√© √† l'**Intelligence Artificielle G√©n√©rative (GenIA)** et au **Traitement Automatique du Langage Naturel (NLP)**. Il est structur√© en deux grandes parties :  
- **Les fondamentaux du NLP en machine learning and Deep learning**
- **L'architecture des Transformers**
- **Des projets basics et avanc√©s bas√©s sur LangChain et les LLMs + deployment**


```markdown
# Concepts de Traitement du Langage Naturel (NLP)

Ce document couvre les techniques fondamentales de pr√©traitement et de repr√©sentation de texte en NLP. Voici une explication d√©taill√©e :

## üìù Pr√©traitement de Texte
- **Objectif** : Pr√©parer le texte brut pour les algorithmes de machine learning
- **√âtapes courantes** :
  - `Tokenisation` (d√©coupage du texte en mots/jetons)
  - `Suppression des stopwords` (mots courants comme "le", "est")
  - `Lemmatisation` (r√©duction des mots √† leur forme de base)
  - `Nettoyage` (gestion des caract√®res sp√©ciaux et ponctuation)

## üî¢ Techniques de Repr√©sentation de Texte

### 1Ô∏è‚É£ Sac de Mots (Bag of Words - BoW)
- **Concept** : Repr√©sente le texte par des fr√©quences de mots
- **Exemple** :
  ```python
  Document 1 : "La nourriture est bonne" ‚Üí {"la":1, "nourriture":1, "est":1, "bonne":1}
  Document 2 : "La nourriture est mauvaise" ‚Üí {"la":1, "nourriture":1, "est":1, "mauvaise":1}
  ```
- **Limitations** :
  - ‚ùå Matrices creuses
  - ‚ùå Pas de sens s√©mantique
  - ‚ùå Probl√®me de mots hors vocabulaire (OOV)

### 2Ô∏è‚É£ TF-IDF (Term Frequency-Inverse Document Frequency)
- **Concept** : Pondere les mots par leur importance relative dans un document
- **Calcul** :
  ```
  TF = (nb d'occurrences du mot dans le doc) / (nb total de mots du doc)
  IDF = log(nb total de docs / nb de docs contenant le mot)
  TF-IDF = TF * IDF
  ```
- **Avantages** :
  - ‚úÖ Capture l'importance des mots
  - ‚úÖ Taille fixe (taille du vocabulaire)

### 3Ô∏è‚É£ Word Embeddings (Word2Vec)
- **Concept** : Repr√©sente les mots comme des vecteurs denses
- **Propri√©t√©s** :
  - Similarit√© s√©mantique pr√©serv√©e
  - Relations du type : roi - homme + femme ‚âà reine
- **Types** :
  - `CBOW` : Pr√©dit le mot cible √† partir du contexte
  - `Skip-gram` : Pr√©dit le contexte √† partir du mot cible
- **Avantages** :
  - ‚úÖ Vecteurs denses
  - ‚úÖ Capture le sens s√©mantique
  - ‚úÖ G√®re les mots OOV

### 4Ô∏è‚É£ Moyenne de Word2Vec
- **Concept** : Repr√©sente des documents par la moyenne des vecteurs de leurs mots
- **Exemple** :
  ```python
  Document : "La nourriture est bonne"
  Vecteur = moyenne(Word2Vec("la"), Word2Vec("nourriture"), ...)
  ```

## üìê Similarit√© Cosinus
- Mesure la similarit√© entre vecteurs
- `Distance = 1 - Similarit√© Cosinus`
- Plage : 0 (diff√©rent) √† 1 (identique)

## üõ† Outils d'Impl√©mentation
- Biblioth√®ques Python :
  - `scikit-learn` (pour BoW, TF-IDF)
  - `Gensim` (pour Word2Vec)
  - `NLTK/spaCy` (pour le pr√©traitement)

## üìä Comparatif des M√©thodes
| M√©thode       | S√©mantique | Densit√© | Taille | OOV  |
|---------------|------------|---------|--------|------|
| BoW           | ‚ùå         | Creuse  | Variable | ‚ùå   |
| TF-IDF        | ‚ö†Ô∏è         | Creuse  | Fixe   | ‚ùå   |
| Word2Vec      | ‚úÖ         | Dense   | Fixe   | ‚úÖ   |

Ce r√©sum√© pr√©sente les techniques essentielles pour convertir du texte en repr√©sentations num√©riques exploitables par le machine learning.
