# üß† GenIA & NLP ‚Äì Bases et Projets avec LangChain

Bienvenue dans ce d√©p√¥t consacr√© √† l'**Intelligence Artificielle G√©n√©rative (GenIA)** et au **Traitement Automatique du Langage Naturel (NLP)**. Il est structur√© en deux grandes parties :  
- **Les fondamentaux du NLP en machine learning and Deep learning**
- **L'architecture des Transformers**
- **Des projets basics et avanc√©s bas√©s sur LangChain et les LLMs + deployment**


# Concepts de Traitement du Langage Naturel (NLP)

Ce document couvre les techniques fondamentales de pr√©traitement et de repr√©sentation de texte en NLP. Voici une explication d√©taill√©e :

## üìù Pr√©traitement de Texte
- **Objectif** : Pr√©parer le texte brut pour les algorithmes de machine learning
- **√âtapes courantes** :
  - `Tokenisation` (d√©coupage du texte en mots/jetons)
  - `Suppression des stopwords` (mots courants comme "le", "est")
  - `Lemmatisation` (r√©duction des mots √† leur forme de base)
  - `Nettoyage` (gestion des caract√®res sp√©ciaux et ponctuation)

## üî¢ Techniques de Repr√©sentation de Texte

### 1Ô∏è‚É£ Sac de Mots (Bag of Words - BoW)
- **Concept** : Repr√©sente le texte par des fr√©quences de mots
- **Exemple** :
  ```python
  Document 1 : "La nourriture est bonne" ‚Üí {"la":1, "nourriture":1, "est":1, "bonne":1}
  Document 2 : "La nourriture est mauvaise" ‚Üí {"la":1, "nourriture":1, "est":1, "mauvaise":1}
  ```
- **Limitations** :
  - ‚ùå Matrices creuses
  - ‚ùå Pas de sens s√©mantique
  - ‚ùå Probl√®me de mots hors vocabulaire (OOV)

### 2Ô∏è‚É£ TF-IDF (Term Frequency-Inverse Document Frequency)
- **Concept** : Pondere les mots par leur importance relative dans un document
- **Calcul** :
  ```
  TF = (nb d'occurrences du mot dans le doc) / (nb total de mots du doc)
  IDF = log(nb total de docs / nb de docs contenant le mot)
  TF-IDF = TF * IDF
  ```
- **Avantages** :
  - ‚úÖ Capture l'importance des mots
  - ‚úÖ Taille fixe (taille du vocabulaire)

### 3Ô∏è‚É£ Word Embeddings (Word2Vec)
- **Concept** : Repr√©sente les mots comme des vecteurs denses
- **Propri√©t√©s** :
  - Similarit√© s√©mantique pr√©serv√©e
  - Relations du type : roi - homme + femme ‚âà reine
- **Types** :
  - `CBOW` : Pr√©dit le mot cible √† partir du contexte
  - `Skip-gram` : Pr√©dit le contexte √† partir du mot cible
- **Avantages** :
  - ‚úÖ Vecteurs denses
  - ‚úÖ Capture le sens s√©mantique
  - ‚úÖ G√®re les mots OOV

### 4Ô∏è‚É£ Moyenne de Word2Vec
- **Concept** : Repr√©sente des documents par la moyenne des vecteurs de leurs mots
- **Exemple** :
  ```python
  Document : "La nourriture est bonne"
  Vecteur = moyenne(Word2Vec("la"), Word2Vec("nourriture"), ...)
  ```

## üìê Similarit√© Cosinus
- Mesure la similarit√© entre vecteurs
- `Distance = 1 - Similarit√© Cosinus`
- Plage : 0 (diff√©rent) √† 1 (identique)

## üõ† Outils d'Impl√©mentation
- Biblioth√®ques Python :
  - `scikit-learn` (pour BoW, TF-IDF)
  - `Gensim` (pour Word2Vec)
  - `NLTK/spaCy` (pour le pr√©traitement)

## üìä Comparatif des M√©thodes
| M√©thode       | S√©mantique | Densit√© | Taille | OOV  |
|---------------|------------|---------|--------|------|
| BoW           | ‚ùå         | Creuse  | Variable | ‚ùå   |
| TF-IDF        | ‚ö†Ô∏è         | Creuse  | Fixe   | ‚ùå   |
| Word2Vec      | ‚úÖ         | Dense   | Fixe   | ‚úÖ   |

Ce r√©sum√© pr√©sente les techniques essentielles pour convertir du texte en repr√©sentations num√©riques exploitables par le machine learning.

# *TRANSFORMERS ---EXPLICATION SIMPLE*
L'Architecture des Transformers Expliqu√©e Simplement
Vue d'ensemble : Qu'est-ce qu'un Transformer ?
Un Transformer est comme un traducteur super intelligent qui peut comprendre et g√©n√©rer du texte. Il est compos√© de deux parties principales :

L'Encodeur (√† gauche) : lit et comprend le texte d'entr√©e
Le D√©codeur (√† droite) : g√©n√®re le texte de sortie

Imaginez que vous donnez une phrase en fran√ßais √† un ami polyglotte, il la comprend (encodeur) puis la traduit en anglais (d√©codeur).
Fonctionnement √©tape par √©tape
1. Input/Output Embeddings (Plongements d'entr√©e/sortie)
R√¥le : Convertir les mots en nombres que l'ordinateur peut comprendre
Analogie : C'est comme donner un code num√©rique √† chaque mot :

"chat" ‚Üí [0.2, 0.8, 0.1, ...]
"chien" ‚Üí [0.3, 0.7, 0.2, ...]
"amour" ‚Üí [0.9, 0.1, 0.8, ...]

Les mots similaires ont des codes similaires.
2. Positional Encoding (Encodage positionnel)
R√¥le : Indiquer la position de chaque mot dans la phrase
Pourquoi c'est important : L'ordre des mots change le sens !

"Le chat mange le poisson" ‚â† "Le poisson mange le chat"

Analogie : C'est comme num√©roter les mots dans une phrase :

"Le¬π chat¬≤ mange¬≥ le‚Å¥ poisson‚Åµ"

3. Multi-Head Attention (Attention multi-t√™tes)
R√¥le : Permettre √† chaque mot de "regarder" et comprendre sa relation avec tous les autres mots
Analogie : Imaginez que vous lisez une phrase et que votre cerveau fait automatiquement des liens :

Dans "Marie donne sa pomme √† Jean", le mot "sa" se r√©f√®re √† "Marie"
L'attention permet au mod√®le de faire ces connexions

Pourquoi "Multi-Head" (plusieurs t√™tes) ? : C'est comme avoir plusieurs types d'attention :

Une t√™te se concentre sur la grammaire
Une autre sur le sens
Une troisi√®me sur les relations entre personnes

4. Masked Multi-Head Attention (Attention masqu√©e)
R√¥le : Emp√™cher le d√©codeur de "tricher" en regardant les mots futurs
Analogie : C'est comme jouer au jeu du "cadavre exquis" o√π vous ne pouvez voir que les mots d√©j√† √©crits, pas ceux qui viennent apr√®s.
Quand le mod√®le g√©n√®re "Le chat mange...", il ne doit pas voir que le mot suivant est "poisson" pour que l'apprentissage soit honn√™te.
5. Add & Norm (Addition et Normalisation)
R√¥le : Stabiliser l'apprentissage et √©viter que les valeurs deviennent trop grandes ou trop petites
Analogie : C'est comme un r√©gulateur dans une voiture qui maintient la vitesse stable :

Add : Combine l'ancienne information avec la nouvelle
Norm : Remet tout √† une √©chelle "normale"

6. Feed Forward (R√©seau de neurones)
R√¥le : Traiter l'information de chaque position ind√©pendamment
Analogie : C'est comme un filtre qui am√©liore chaque mot individuellement :

Prend un mot avec son contexte
Le "polit" et l'am√©liore
Le renvoie plus "intelligent"

7. Linear + Softmax (Couche finale)
R√¥le : Convertir les nombres finaux en probabilit√©s pour chaque mot possible
Analogie : C'est comme un vote pour d√©cider quel mot vient ensuite :

"chat" : 60% de chance
"chien" : 30% de chance
"oiseau" : 10% de chance

Flux complet d'information
Dans l'Encodeur :

Mots ‚Üí Embeddings (conversion en nombres)
+ Positional Encoding (ajout de la position)
Multi-Head Attention (les mots se "parlent" entre eux)
Add & Norm (stabilisation)
Feed Forward (traitement individuel)
Add & Norm (stabilisation finale)

Dans le D√©codeur :

Mots de sortie ‚Üí Embeddings
+ Positional Encoding
Masked Multi-Head Attention (attention sur les mots d√©j√† g√©n√©r√©s)
Add & Norm
Multi-Head Attention (attention sur l'encodeur)
Add & Norm
Feed Forward
Add & Norm
Linear + Softmax (choix du prochain mot)

Pourquoi cette architecture est r√©volutionnaire ?
1. Parall√©lisation

Contrairement aux anciens mod√®les qui lisaient mot par mot
Les Transformers peuvent traiter tous les mots en m√™me temps
Analogie : C'est comme lire une page enti√®re d'un coup au lieu de lettre par lettre

2. Attention globale

Chaque mot peut "voir" tous les autres mots
Permet de comprendre des relations complexes
Analogie : C'est comme avoir une vue d'ensemble d'un puzzle au lieu de voir pi√®ce par pi√®ce

3. Flexibilit√©

Peut traiter des phrases de longueur variable
S'adapte automatiquement au contexte
Analogie : C'est comme un √©lastique qui s'adapte √† ce qu'on y met

Exemples concrets d'utilisation
Traduction

Entr√©e : "Hello, how are you?"
Sortie : "Bonjour, comment allez-vous ?"

G√©n√©ration de texte

Entr√©e : "Il √©tait une fois"
Sortie : "Il √©tait une fois un royaume lointain o√π vivait une princesse..."

R√©sum√© de texte

Entr√©e : Long article de presse
Sortie : R√©sum√© en 3 phrases

M√©taphore globale
Imaginez un orchestre symphonique :

Les musiciens = les mots
Le chef d'orchestre = l'attention (coordonne tout le monde)
Les partitions = les embeddings (donnent les instructions)
La disposition sur sc√®ne = l'encodage positionnel
Le concert final = la sortie du mod√®le

Chaque musicien (mot) √©coute les autres (attention), suit sa partition (embedding), conna√Æt sa place (position), et ensemble ils cr√©ent une symphonie (texte coh√©rent).
Analogie avec la lecture humaine
Quand vous lisez cette phrase : "Marie a donn√© sa pomme rouge √† Jean car il avait faim"
Votre cerveau fait automatiquement :

Embedding : Comprend chaque mot
Position : Sait l'ordre des mots
Attention : Comprend que "sa" = Marie, "il" = Jean, "rouge" = pomme
Contexte : Comprend la relation cause-effet entre la faim et le don

Les Transformers imitent ce processus naturel !