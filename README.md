# ğŸ§  GenIA & NLP â€“ Bases et Projets avec LangChain

Bienvenue dans ce dÃ©pÃ´t consacrÃ© Ã  l'**Intelligence Artificielle GÃ©nÃ©rative (GenIA)** et au **Traitement Automatique du Langage Naturel (NLP)**. Il est structurÃ© en deux grandes parties :  
- **Les fondamentaux du NLP en machine learning and Deep learning**
- **L'architecture des Transformers**
- **Des projets basics et avancÃ©s basÃ©s sur LangChain et les LLMs + deployment**


# Concepts de Traitement du Langage Naturel (NLP)

Ce document couvre les techniques fondamentales de prÃ©traitement et de reprÃ©sentation de texte en NLP. Voici une explication dÃ©taillÃ©e :

## ğŸ“ PrÃ©traitement de Texte
- **Objectif** : PrÃ©parer le texte brut pour les algorithmes de machine learning
- **Ã‰tapes courantes** :
  - `Tokenisation` (dÃ©coupage du texte en mots/jetons)
  - `Suppression des stopwords` (mots courants comme "le", "est")
  - `Lemmatisation` (rÃ©duction des mots Ã  leur forme de base)
  - `Nettoyage` (gestion des caractÃ¨res spÃ©ciaux et ponctuation)

## ğŸ”¢ Techniques de ReprÃ©sentation de Texte

### 1ï¸âƒ£ Sac de Mots (Bag of Words - BoW)
- **Concept** : ReprÃ©sente le texte par des frÃ©quences de mots
- **Exemple** :
  ```python
  Document 1 : "La nourriture est bonne" â†’ {"la":1, "nourriture":1, "est":1, "bonne":1}
  Document 2 : "La nourriture est mauvaise" â†’ {"la":1, "nourriture":1, "est":1, "mauvaise":1}
  ```
- **Limitations** :
  - âŒ Matrices creuses
  - âŒ Pas de sens sÃ©mantique
  - âŒ ProblÃ¨me de mots hors vocabulaire (OOV)

### 2ï¸âƒ£ TF-IDF (Term Frequency-Inverse Document Frequency)
- **Concept** : Pondere les mots par leur importance relative dans un document
- **Calcul** :
  ```
  TF = (nb d'occurrences du mot dans le doc) / (nb total de mots du doc)
  IDF = log(nb total de docs / nb de docs contenant le mot)
  TF-IDF = TF * IDF
  ```
- **Avantages** :
  - âœ… Capture l'importance des mots
  - âœ… Taille fixe (taille du vocabulaire)

### 3ï¸âƒ£ Word Embeddings (Word2Vec)
- **Concept** : ReprÃ©sente les mots comme des vecteurs denses
- **PropriÃ©tÃ©s** :
  - SimilaritÃ© sÃ©mantique prÃ©servÃ©e
  - Relations du type : roi - homme + femme â‰ˆ reine
- **Types** :
  - `CBOW` : PrÃ©dit le mot cible Ã  partir du contexte
  - `Skip-gram` : PrÃ©dit le contexte Ã  partir du mot cible
- **Avantages** :
  - âœ… Vecteurs denses
  - âœ… Capture le sens sÃ©mantique
  - âœ… GÃ¨re les mots OOV

### 4ï¸âƒ£ Moyenne de Word2Vec
- **Concept** : ReprÃ©sente des documents par la moyenne des vecteurs de leurs mots
- **Exemple** :
  ```python
  Document : "La nourriture est bonne"
  Vecteur = moyenne(Word2Vec("la"), Word2Vec("nourriture"), ...)
  ```

## ğŸ“ SimilaritÃ© Cosinus
- Mesure la similaritÃ© entre vecteurs
- `Distance = 1 - SimilaritÃ© Cosinus`
- Plage : 0 (diffÃ©rent) Ã  1 (identique)

## ğŸ›  Outils d'ImplÃ©mentation
- BibliothÃ¨ques Python :
  - `scikit-learn` (pour BoW, TF-IDF)
  - `Gensim` (pour Word2Vec)
  - `NLTK/spaCy` (pour le prÃ©traitement)

## ğŸ“Š Comparatif des MÃ©thodes
| MÃ©thode       | SÃ©mantique | DensitÃ© | Taille | OOV  |
|---------------|------------|---------|--------|------|
| BoW           | âŒ         | Creuse  | Variable | âŒ   |
| TF-IDF        | âš ï¸         | Creuse  | Fixe   | âŒ   |
| Word2Vec      | âœ…         | Dense   | Fixe   | âœ…   |

Ce rÃ©sumÃ© prÃ©sente les techniques essentielles pour convertir du texte en reprÃ©sentations numÃ©riques exploitables par le machine learning.

# ğŸŒ TRANSFORMERS â€” EXPLICATION SIMPLE

## ğŸ§  Vue d'ensemble : Qu'est-ce qu'un Transformer ?

Un **Transformer** est comme un traducteur super intelligent qui peut comprendre et gÃ©nÃ©rer du texte. Il est composÃ© de deux parties principales :

- **L'Encodeur** (Ã  gauche) : lit et comprend le texte d'entrÃ©e  
- **Le DÃ©codeur** (Ã  droite) : gÃ©nÃ¨re le texte de sortie  

> ğŸ¯ Imaginez que vous donnez une phrase en franÃ§ais Ã  un ami polyglotte : il la comprend (encodeur), puis la traduit en anglais (dÃ©codeur).

---

## âš™ï¸ Fonctionnement Ã©tape par Ã©tape

### 1ï¸âƒ£ Input/Output Embeddings (Plongements d'entrÃ©e/sortie)

**RÃ´le** : Convertir les mots en vecteurs numÃ©riques que l'ordinateur comprend.  
**Analogie** : Chaque mot reÃ§oit un "code" unique :

- "chat" â†’ [0.2, 0.8, 0.1, ...]  
- "chien" â†’ [0.3, 0.7, 0.2, ...]  
- "amour" â†’ [0.9, 0.1, 0.8, ...]

Les mots proches en sens ont des vecteurs similaires.

---

### 2ï¸âƒ£ Positional Encoding (Encodage positionnel)

**RÃ´le** : Indiquer la position de chaque mot dans la phrase.  
**Pourquoi ?** : Lâ€™ordre change le sens !

- "Le chat mange le poisson" â‰  "Le poisson mange le chat"  
**Analogie** : On numÃ©rote chaque mot :

- "LeÂ¹ chatÂ² mangeÂ³ leâ´ poissonâµ"

---

### 3ï¸âƒ£ Multi-Head Attention (Attention multi-tÃªtes)

**RÃ´le** : Chaque mot "regarde" les autres pour comprendre le contexte.  
**Analogie** : Votre cerveau Ã©tablit des liens :

- Dans "Marie donne sa pomme Ã  Jean", le mot **"sa"** fait rÃ©fÃ©rence Ã  **"Marie"**.

**Pourquoi plusieurs tÃªtes ?**

- Une tÃªte analyse la grammaire  
- Une autre le sens  
- Une autre les relations entre entitÃ©s

---

### 4ï¸âƒ£ Masked Multi-Head Attention (DÃ©codeur uniquement)

**RÃ´le** : EmpÃªcher le modÃ¨le de "voir le futur" lors de la gÃ©nÃ©ration.  
**Analogie** : Comme le jeu du "cadavre exquis" â€” on ne voit que les mots dÃ©jÃ  gÃ©nÃ©rÃ©s.

---

### 5ï¸âƒ£ Add & Norm (Addition et Normalisation)

**RÃ´le** : Stabiliser les valeurs pendant lâ€™apprentissage.  
**Analogie** : Un rÃ©gulateur dans une voiture :

- `Add` : Ajoute les nouvelles infos  
- `Norm` : Normalise lâ€™Ã©chelle

---

### 6ï¸âƒ£ Feed Forward (RÃ©seau de neurones)

**RÃ´le** : AmÃ©liorer individuellement chaque mot avec son contexte.  
**Analogie** : Comme un filtre qui polit chaque mot.

---

### 7ï¸âƒ£ Linear + Softmax (Couche finale)

**RÃ´le** : PrÃ©dire le mot suivant en gÃ©nÃ©rant des probabilitÃ©s.  
**Exemple** :

- "chat" : 60%  
- "chien" : 30%  
- "oiseau" : 10%

---

## ğŸ”„ Flux dâ€™information complet

### ğŸ“˜ Dans l'Encodeur :

